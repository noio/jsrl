<html>
    <head>
    	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min.js"></script>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/flot/0.8/jquery.flot.min.js"></script>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/flot/0.8/jquery.flot.errorbars.min.js"></script>
        <script type="text/javascript" src="../../js/lib/klass.min.js"></script>

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <script type="text/javascript" src="../../js/lib/codemirror.min.js"></script>
        <link type="text/css" href="../../js/lib/codemirror.css" media="all" rel="stylesheet">

        <script type="text/javascript" src="../../js/utils.js"></script>
        <script type="text/javascript" src="../../js/learning.js"></script>
        <script type="text/javascript" src="../../js/gridworld.js"></script>
        <link href="../../js/gridworld.css" media="all" rel="stylesheet" type="text/css">

        <script type="text/javascript" src="../../js/projector.js"></script>
        
        <link href="../../style.css" media="all" rel="stylesheet" type="text/css">

        <title>Cognitie Les 3</title>

    </head>
    <body>
        <h1>Practicum Cognitie</h1>
        <div id='projector-description'>
			<h2>Q learning</h2>
			<p>Je hebt de afgelopen 2 lessen gekeken naar het n-armed bandits problem.
			Dit is een probleem waarbij de agent probeert zijn <em>acties</em> zo te kiezen,
			dat deze binnen een beperkt aantal stappen maximale reward binnensleept.
			De enige variabele die daarbij een rol speelt is de actie: "neem ik
			de bandit in het noorden, oosten, zuiden of westen."
			Dit is een probleem met een <em>action-space</em>, met slechts vier opties.</p>

			<p>Om complexere problemen op te lossen, zoals bijvoorbeeld <a href="http://www.youtube.com/watch?v=CLIPLiQDIk0" target="_blank">efficient een kamer schoon zuigen</a> of <a href="http://www.youtube.com/watch?v=3mvE_CeH9q0" target="_blank">een helikopter te besturen</a> (zie ook <a href="http://www.youtube.com/watch?v=r4l4zLrQx-I" target="_blank">hier</a>), hangt de
			actie die je wilt nemen af van de omgeving. Als je met een helikopter vliegt, kun je moeilijk
			volhouden dat naar rechts vliegen altijd wat meer reward oplevert dan naar links, zoals met de bandits.</p>

			<p>Wat je nodig hebt is een <em>state space</em>: een representatie van de huidige staat
			van de agent en de wereld, die zich in verschillende toestanden kunnen bevinden en daarmee om verschillende 
			acties vragen. Je Reinforcement Learning algoritme zal niet slechts moeten moeten leren om de juiste
			actie te kiezen in <em>action-space</em>, het zal moeten leren de juiste actie te nemen, gegeven
			een bepaalde state van de wereld. Hij moet leren in een <em>state-action</em>-space.</p>
			
			<!--<p> Waar de <em>action-space</em> er in de vorige 2 lessen nog zo uit zag:</p>

			<table style='color: black; font-size: 14'>
			<tr><td></td><td>Noord</td><td>Oost</td><td>Zuid</td><td>West</td></tr>
			<tr><td></td><td>0.4</td><td>0.6</td><td>0.5</td><td>0.45</td></tr>
			</table>

			<p>Heb je nu een <em>state-action</em>-space, waar de space
			in dit geval casino's zijn:</p>
			<table style='color: black; font-size: 14'>
			<tr><td></td><td>Noord</td><td>Oost</td><td>Zuid</td><td>West</td><td>Ga naar ander casino</td></tr>
			<tr><td>Casino1</td><td>0.2</td><td>0.4</td><td>0.6</td><td>0.5</td><td>0.45</td></tr>
			<tr><td>Casino2</td><td>0.1</td><td>0.3</td><td>0.2</td><td>0.1</td><td>0.7</td></tr>
			</table>-->

<!-- 			<p>\(Q\) is nu dus ook geen functie meer van alleen \(a\):
			$$Q(a)$$
			 maar van \(s\) &#233;n \(a\):</p>

			$$Q(s, a)$$

			Belangrijk hierbij is dat als je een bepaalde actie in een bepaalde state uitvoert,
			je terecht komt in <em>een volgende state</em> \(s_{t+1}\).</p> 
		-->

			<h3>De Mars Rover</h3>
			<p>
				In deze les is ons probleem een simulatie van een van NASA's <a href="http://en.wikipedia.org/wiki/Mars_rover">Mars rovers</a>, die onderzoek doet op Mars. De robot kan vier verschillende kanten op rijden, en zijn state (\(s_t\)) wordt gegeven door de huidige positie. Afhankelijk van de actie die de robot kiest komt hij in een nieuwe state \(s_{t+1}\). Door in elke state \(s\) de goede actie \(a\) te kiezen moet de robot een weg vinden naar een <em>research site</em>, waar misschien wel water of leven is!
				De robot krijgt pas een reward als hij de research site vindt, maar als hij tegen een obstakel aan rijdt krijgt hij een reward van -1. Als de robot de research site bereikt krijgt hij een reward gelijk aan \(100 - t\). De reward is dus hoger als de robot sneller het einde bereikt.
			</p>


			<p>
				We moeten nu dus een schatting bijhouden van de verwachte waarden van onze acties in <em>elke</em> state, dus \(Q(a)\) wordt \(Q(s,a)\). Maar omdat we soms acties willen uitvoeren die leiden tot meer reward in de toekomst, bevat \(Q(s,a)\) niet alleen de verwachting van de reward in state \(s\), maar ook een schatting van de <em>toekomstige</em> reward:

				$$Q(s, a) = E[r_{s_t,a_t}] + \max_{a_{t+1}} Q(s_{t+1}, a_{t+1})$$

			</p>
			
	<!-- 		De update-functie wordt dus een stukje ingewikelder dan in de vorige lessen:
			je wilt voor elke actie \(a\) gegeven een state \(s\) de gemiddelde
			reward die je als gevolg van de actie verwacht:</p>

			$$E[r_{a}]$$

			<p><em>plus</em> de reward die je in de toekomst verwacht, gegeven
			de state waar je in terecht komt, als je die actie neemt.
			Daarvoor gebruik je de Q-value van de volgende state \(s_{t+1}\), gegeven dat
			je in die state de best mogelijke actie \(a\) kiest:</p>

			$$\max_a Q(s_{t+1}, a)$$

			<p>Deze reward in de toekomst is echter misschien onzeker, dus wil je 
			hem `discounten' in de tijd met een factor \(\gamma\):

			$$\gamma \max_a Q(s_{t+1}, a)$$

			De gehele \(Q\)-functie ziet er nu dus zo uit:
			$$Q(s, a) = E[r_{a}] +  \gamma \max_a Q(s_{t+1}, a)$$
			</p>
 -->

			<p>
				Hoe berekenen we deze functie nu? Als de robot aan het probleem begint weet deze nog helemaal niks. We gebruiken <a href="http://en.wikipedia.org/wiki/Temporal_difference_learning">temporal difference learning</a>: elke keer dat we een actie doen, kijken we hoeveel reward we daadwerkelijk krijgen (\(r_t\)) en wat de beste verwachte reward in de volgende state is. Deze tellen we bij elkaar op voor een schatting van de waarde \(Q(s,a)\).
			
				$$r_{t} + \max_a Q(s_{t+1}, a)$$

				Het verschil tussen deze geschatte waarde en de huidige waarde is de <em>temporal difference error</em> \(\delta\) (delta).

				$$\delta = r_{t} + \max_a Q(s_{t+1}, a) - Q(s,a) $$

				Aan de hand van dit verschil passen we de waarde van \(Q(s, a)\) aan. Omdat het mogelijk is dat we niet altijd dezelfde reward krijgen voor een actie, passen we \(Q(s,a)\) langzaam aan richting de schatting, met behulp van de <em>learning rate</em> \(\alpha\). Hoe hoger we \(\alpha\) instellen, hoe sneller de robot zijn gedrag aanpast.

				$$Q(s, a) = Q(s, a) + \alpha \cdot \delta $$

				Vergelijk deze formule even met de `update' functie rechts.


			<p class="question"> Druk een keer op `run'. Wat zie je gebeuren?</p>
			<p class="question"> Klik op een vakje om te zien wat de waarden van \(Q(s,a)\) zijn voor een paar states. Wat betekenen de witte pijltjes?</p> 
			<p class="question"> Voor elke state heb je vier state-action waarden (&#233;&#233;n voor elke mogelijke actie). 
			Waarom krijgt elke state dan maar &#233;&#233;n kleurtje? Waar staat dit kleurtje voor
			denk je?</p>
			<!-- <p class="question"><span>1c)</span> Stel Alpha eens in op verschillende waarden: 0.1, 0.5 en 1.0. Wat valt je op?</p> -->
			
			<!-- <p class="question"><span>2a)</span> Wat gebeurt er al je gamma op 1 zet? Hoe komt dit?</p> -->
			
			<!-- <p class="question"><span>2b)</span> In dit probleem krijgt de agent een reward op het moment
				dat hij het einddoel bereikt en een negatieve reward STEP_REWARD op het moment dat 
				hij het einddoel niet heeft bereikt. Wat gebeurt er als je deze STEP_REWARD op 0 zet? Hoe komt dit?</p> -->
			De robot vertrouwt op het feit dat hij in elke staat wel eens een willekeurige actie kiest (exploration), om zo toevallig de research site te vinden. In plaats daarvan kunnen we een techniek gebruiken die <em>optimistic initialization</em> heet. In plaats van de Q-tabel te initializeren met waarde 0, kunnen we hier een hogere waarde in zetten. Elke keer als de robot een actie kiest, zal hij daarom "teleurgesteld" zijn met het resultaat. Immers, zijn huidige actie leverde waarschijnlijk niks op, maar de schatting in \(Q(s,a)\) bevatte wel een hogere waarde. De robot zal dus telkens acties kiezen die hij nog niet geprobeerd heeft, omdat hij de 'hoop' heeft dat die acties een reward opleveren: hij is optimistisch.

			<p class="question">Implementeer <em>optimistic initialization</em> door de Q-tabel te vullen met een hogere waarde, bijvoorbeeld 10.</p>
			<p class="question">Zet Epsilon eens op 0 en initialiseer de Q-tabel zowel met 10 als met 0. 
				Wanneer vindt de robot de research site? Hoe kan dit?</p>
				
			<p class="question">Draai 5 simulaties mét en zónder <em>optimistic initialization</em>. Zet \(\epsilon\) op 0.5 als je géén optimistic initialization gebruikt.
				(hint: met 'Play All' bespaar je tijd). 
				Vergelijk de gemiddelde return per tijdstap (stop de resultaten van 'Get Data' in een grafiek).</p>
				
			<a href="lesson04.html">Ga naar les 4</a>
        </div>

<script id="projector-script">
/**
 * This will run immediately upon load.
 * Put any variables you want to save into 'my'.
 */

var setup = function(my){
	my.panels = projector.createPanels([1,1,1]);
	my.buttons = projector.createButtons(["Play", "Play All", "Get Data"])
	
	var task_world = [
	"_ _ _ _ _ _ _ _ w w _ _ _ _ w",
	"_ _ _ w _ _ _ _ _ w _ _ _ _ _",
	"_ _ s _ _ _ w w _ _ _ _ _ _ w",
	"_ _ _ _ _ _ _ w _ w w _ z _ _",
	"_ _ _ _ w _ _ _ _ _ w _ _ _ _",
	"_ _ _ w _ _ _ _ _ _ _ _ _ _ _"
	]
	my.task = new gridworld.GridWorld(task_world)
	my.task.setpanel(my.panels[1])
	my.task.STEP_REWARD = 0;
	my.task.REWARD_TIMESTEPS = true;
	my.task.MAX_STEPS = 100;
	my.task.render();
	
	my.task.onTileClick.push(function(state){
		var statevals = my.Q.get(state)
      	plotActions(my.panels[2], statevals);
      	console.log("State: " + state + " Best action: " + argmax(statevals) + ": " + valmax(statevals));
	});

    my.buttons['Get Data'].on('click', function(){
        showDataTable(my.R);
    });
}

var first = function(my){
	my.play = true;
	my.playAll = false;

	var ALPHA
	var EPSILON
	//:edit {"title":"Variables"}
	var task_world = [
	"_ _ _ _ _ _ _ _ w w _ _ _ _ w",
	"_ _ _ w _ _ _ _ _ w _ _ _ _ _",
	"_ _ s _ _ _ w w _ _ _ _ _ _ w",
	"_ _ _ _ _ _ _ w _ w w _ z _ _",
	"_ _ _ _ w _ _ _ _ _ w _ _ _ _",
	"_ _ _ w _ _ _ _ _ _ _ _ _ _ _"
	]
	ALPHA = 0.5
	EPSILON = 0.5
	//:end edit
	my.ALPHA = ALPHA
	my.EPISODES = 200
	my.task.parseWorld(task_world);

	my.Q = new StateActionValueTable()
	var Q = my.Q;
	var task = my.task;

	//:edit {"title":"Initialization"}
	Q.fill(task.states(), task.actions(), 0.0)
	//:end edit

	// Other persistent variables
	my.episode = 0
    my.R = []  // Array with performance data (average return)
    my.steps = []

    $.plot(my.panels[0], [])
    my.task.render(my.Q);

	// Define functions

	my.action_select = function(s){
		var a
		var As = Q.get(s)
		//:edit {"title":"Action Selection"}
		if (chance(EPSILON))
			a = randompick(As);
		else
			a = argmax(As);
		//:end edit
		return a

	}

	my.update = function(s, a, r, s_){
		var difference
		var new_Q
		//:edit {"title":"Update"}
		delta     = r + 0.9 * valmax(Q.get(s_)) - Q.get(s, a)
		new_value = Q.get(s, a) + ALPHA * delta
		Q.set(s, a, new_value)
		//:end
	}

	my.start_episode = function(){
		my.task.reset();
	}

	my.run_episode = function(){
		my.start_episode();
		while(!task.ended()){
			my.do_step();
		}
		my.start_episode()
	}

	my.do_step = function(){
		var s = task.getState();
		var a = my.action_select(s)
		var r = task.act(a);
		var s_ = task.getState();

		my.update(s, a, r, s_);

		if (task.ended()){
			console.log("Episode finished in " + my.task.step + " steps, return: " + my.task.R + ".");

			my.episode ++;
	        
	        my.R.push([my.episode, my.task.R]);
	        my.steps.push([my.episode, my.task.step])
	        $.plot(my.panels[0], [{data:my.R, label:'Return', color:'#4BB04D'}]) // {data:my.steps, label:'Timesteps'}]);
		}
	}
}

var run = function(my, run_num){

	var show_every = 20

	if(my.episode % show_every == 0 && !my.playAll)
	{
		if (my.buttons['Play'].attr('data-justclicked') == 'true') {
			my.play = true;
			my.buttons['Play'].addClass('disabled');
		}

		if (my.buttons['Play All'].attr('data-justclicked') == 'true') {
			my.playAll = my.play = true;			
			my.buttons['Play'].addClass('disabled');
			my.buttons['Play All'].addClass('disabled');
		}

		if (my.play && run_num % 5 == 0) {
			my.do_step();
			my.task.render(my.Q);
		}

	} else {
		my.buttons['Play'].removeClass('disabled');
		my.play = my.playAll;
		my.run_episode();
		my.task.render(my.Q);
	}

	if (my.episode >= my.EPISODES){
		return true;
	}
}
</script>

<script>
    prj = new projector.Projector($('body'));
</script>

</body>
</html>