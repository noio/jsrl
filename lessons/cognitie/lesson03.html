<html>
    <head>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min.js"></script>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/flot/0.8/jquery.flot.min.js"></script>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/flot/0.8/jquery.flot.errorbars.min.js"></script>
        <script type="text/javascript" src="../../js/lib/klass.min.js"></script>

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <script type="text/javascript" src="../../js/lib/codemirror.min.js"></script>
        <link type="text/css" href="../../js/lib/codemirror.css" media="all" rel="stylesheet">

        <script type="text/javascript" src="../../js/utils.js"></script>
        <script type="text/javascript" src="../../js/learning.js"></script>
        <script type="text/javascript" src="../../js/gridworld.js"></script>
        <link href="../../js/gridworld.css" media="all" rel="stylesheet" type="text/css">

        <script type="text/javascript" src="../../js/projector.js"></script>
        
        <link href="../../style.css" media="all" rel="stylesheet" type="text/css">

        <title>Cognitie Les 3</title>

    </head>
    <body>
        <h1>Practicum Cognitie</h1>
        <div id='projector-description'>
			<h2>Q learning</h2>
			<p>Je hebt de afgelopen 2 lessen gekeken naar het n-armed bandits problem.
			Dit is een probleem waarbij de agent probeert zijn <em>acties</em> zo te kiezen,
			dat deze binnen een beperkt aantal stappen maximale reward binnensleept.
			De enige variabele die daarbij een rol speelt is de actie: "neem ik
			de bandit in het noorden, oosten, zuiden of westen."
			Dit is een probleem met een <em>action-space</em>, met slechts vier opties.</p>

			<p>Om complexere problemen op te lossen, zoals bijvoorbeeld <a href="http://www.youtube.com/watch?v=CLIPLiQDIk0">efficient een kamer schoon zuigen</a> of <a href="http://www.youtube.com/watch?v=3mvE_CeH9q0">een helikopter te besturen</a>, hangt de
			actie die je wilt nemen af van de omgeving (zie ook <a href="http://www.youtube.com/watch?v=r4l4zLrQx-I">hier</a>). Als je met een helikopter vliegt, kun je moeilijk
			volhouden dat naar rechts vliegen altijd wat meer reward oplevert dan naar links, zoals met de bandits.</p>

			<p>Wat je nodig hebt is een <em>state space</em>: een representatie van de huidige staat
			van de agent en de wereld, die zich in verschillende toestanden kunnen bevinden en daarmee om verschillende 
			acties vragen. Je Reinforcement Learning algoritme zal niet slechts moeten moeten leren om de juiste
			actie te kiezen in <em>action-space</em>, het zal moeten leren de juiste actie te nemen, gegeven
			een bepaalde state van de wereld. Hij moet leren in een <em>state-action</em>-space.</p>
			
			<!--<p> Waar de <em>action-space</em> er in de vorige 2 lessen nog zo uit zag:</p>

			<table style='color: black; font-size: 14'>
			<tr><td></td><td>Noord</td><td>Oost</td><td>Zuid</td><td>West</td></tr>
			<tr><td></td><td>0.4</td><td>0.6</td><td>0.5</td><td>0.45</td></tr>
			</table>

			<p>Heb je nu een <em>state-action</em>-space, waar de space
			in dit geval casino's zijn:</p>
			<table style='color: black; font-size: 14'>
			<tr><td></td><td>Noord</td><td>Oost</td><td>Zuid</td><td>West</td><td>Ga naar ander casino</td></tr>
			<tr><td>Casino1</td><td>0.2</td><td>0.4</td><td>0.6</td><td>0.5</td><td>0.45</td></tr>
			<tr><td>Casino2</td><td>0.1</td><td>0.3</td><td>0.2</td><td>0.1</td><td>0.7</td></tr>
			</table>-->

<!-- 			<p>\(Q\) is nu dus ook geen functie meer van alleen \(a\):
			$$Q(a)$$
			 maar van \(s\) &#233;n \(a\):</p>

			$$Q(s, a)$$

			Belangrijk hierbij is dat als je een bepaalde actie in een bepaalde state uitvoert,
			je terecht komt in <em>een volgende state</em> \(s_{t+1}\).</p> 
		-->

			<h3>De Mars Rover</h3>
			<p>
				In deze les is ons probleem een simulatie van een van NASA's <a href="http://en.wikipedia.org/wiki/Mars_rover">Mars rovers</a>, die onderzoek doet op Mars. De robot kan vier verschillende kanten op rijden, en zijn state (\(s_t\)) wordt gegeven door de huidige positie. Afhankelijk van de actie die de robot kiest komt hij in een nieuwe state \(s_{t+1}\). Door in elke state \(s\) de goede actie \(a\) te kiezen moet de robot een weg vinden naar een <em>research site</em>, waar misschien wel water of leven is!
				De agent krijgt pas een reward als hij de research site vindt, maar als hij tegen een obstakel aan rijdt krijgt hij een reward van \(-1\).
			</p>


			<p>
				We moeten nu dus een schatting bijhouden van de verwachte waarden van onze acties in <em>elke</em> state, dus \(Q(a)\) wordt \(Q(s,a)\). Maar omdat we soms acties willen uitvoeren die leiden tot meer reward in de toekomst, bevat \(Q(s,a)\) niet alleen de verwachting van de reward in state \(s\), maar ook een schatting van de <em>toekomstige</em> reward:

				$$Q(s, a) = E[r_{s,a}] + \max_a Q(s_{t+1}, a)$$

			</p>
			
	<!-- 		De update-functie wordt dus een stukje ingewikelder dan in de vorige lessen:
			je wilt voor elke actie \(a\) gegeven een state \(s\) de gemiddelde
			reward die je als gevolg van de actie verwacht:</p>

			$$E[r_{a}]$$

			<p><em>plus</em> de reward die je in de toekomst verwacht, gegeven
			de state waar je in terecht komt, als je die actie neemt.
			Daarvoor gebruik je de Q-value van de volgende state \(s_{t+1}\), gegeven dat
			je in die state de best mogelijke actie \(a\) kiest:</p>

			$$\max_a Q(s_{t+1}, a)$$

			<p>Deze reward in de toekomst is echter misschien onzeker, dus wil je 
			hem `discounten' in de tijd met een factor \(\gamma\):

			$$\gamma \max_a Q(s_{t+1}, a)$$

			De gehele \(Q\)-functie ziet er nu dus zo uit:
			$$Q(s, a) = E[r_{a}] +  \gamma \max_a Q(s_{t+1}, a)$$
			</p>
 -->

			<p>
				Hoe berekenen we deze functie nu? Als de robot aan het probleem begint weet deze nog helemaal niks. We gebruiken <a href="http://en.wikipedia.org/wiki/Temporal_difference_learning">temporal difference learning</a>: elke keer dat we een actie doen, kijken we hoeveel reward we daadwerkelijk krijgen (\(r_t\)) en wat de beste verwachte reward in de volgende state is. Deze tellen we bij elkaar op voor een schatting van de waarde \(Q(s,a)\).
			
				$$r_{t} + \max_a Q(s_{t+1}, a)$$

				Het verschil tussen deze geschatte waarde en de huidige waarde is de <em>temporal difference error</em> \(\delta\) (delta).

				$$\delta = r_{t} + \max_a Q(s_{t+1}, a) - Q(s,a) $$

				Aan de hand van dit verschil passen we de waarde van \(Q(s, a)\) aan. Omdat het mogelijk is dat we niet altijd dezelfde reward krijgen voor een actie, passen we \(Q(s,a)\) langzaam aan richting de schatting, met behulp van de <em>learning rate</em> \(\alpha\). Hoe hoger we \(\alpha\) instellen, hoe sneller de agent zijn gedrag aanpast.

				$$Q(s, a) = Q(s, a) + \alpha \cdot \delta $$

				Vergelijk deze formule even met de `update' functie rechts.


			<p class="question"> Druk een keer op `run'. Wat zie je gebeuren? Wat betekenen de witte pijltjes?</p>
			<p class="question"><span>1b)</span> Voor elke state heb je vier state-action waarden (&#233;&#233;n voor elke mogelijke actie). 
			Waarom krijgt elke state dan maar &#233;&#233;n kleurtje? Waar staat dit kleurtje voor
			denk je?</p>
			<p class="question"><span>1c)</span> Stel Alpha eens in op verschillende waarden: 0.1, 0.5 en 1.0. Wat valt je op?</p>
			<p class="question"><span>1d)</span> Zet Epsilon eens op 0. Hoe gedraagt de agent zich nu anders? Hoe komt dit?</p>
			
			<!-- <p class="question"><span>2a)</span> Wat gebeurt er al je gamma op 1 zet? Hoe komt dit?</p> -->
			
			<!-- <p class="question"><span>2b)</span> In dit probleem krijgt de agent een reward op het moment
				dat hij het einddoel bereikt en een negatieve reward STEP_REWARD op het moment dat 
				hij het einddoel niet heeft bereikt. Wat gebeurt er als je deze STEP_REWARD op 0 zet? Hoe komt dit?</p> -->

			<p class="question">Iets over optimistic initialization</p>
				
				
			<a href="lesson04.html">Ga naar les 4</a>
        </div>

<script id="projector-script">
/**
 * This will run immediately upon load.
 * Put any variables you want to save into 'my'.
 */

var setup = function(my){
	my.panels = projector.createPanels([1,1,1]);
	my.buttons = projector.createButtons(["Next", "Play"])
	
	var task_world = [
	"_ _ _ _ _ _ _ _ _ w _ _ _ _ _",
	"_ _ _ w _ _ _ _ _ w _ _ _ _ _",
	"_ _ s _ _ _ w w _ _ _ _ _ _ _",
	"_ _ _ _ _ _ _ w _ w w _ z _ _",
	"_ _ _ _ w _ _ _ _ _ w _ _ _ _",
	"_ _ _ w _ _ _ _ _ _ _ _ _ _ _"
	]
	my.task = new gridworld.GridWorld(task_world)
	my.task.setpanel(my.panels[1])
	my.task.STEP_REWARD = 0;
	my.task.REWARD_TIMESTEPS = true;
	my.task.MAX_STEPS = 100;
	my.task.render();
	
	my.autoplay = false

	my.task.panel.find('.tile.overlay').on('click', function(el){
		var state = $(el.currentTarget).attr('data-coord');
      	plotActions(my.panels[2], my.Q.get(state));
      	console.log(state);
    });
}

var first = function(my){
	var ALPHA
	var GAMMA
	var EPSILON

	//:edit {"title":"Variables"}
	ALPHA = 0.5
	EPSILON = 0.4
	//:end edit

	var EPISODES = 500;
	my.ALPHA = ALPHA
	my.GAMMA = GAMMA
	my.EPISODES = EPISODES

	my.Q = new StateActionValueTable()
	var Q = my.Q;
	var task = my.task;

	//:edit {"title":"Initialization"}
	Q.fill(task.states(), task.actions(), 0.0)
	//:end edit

	// Other persistent variables
	my.episode = 0
    my.R = []  // Array with performance data (average return)
    my.steps = []

    $.plot(my.panels[0], [])

	// Define functions

	my.action_select = function(s){
		var a
		var As = Q.get(s)
		//:edit {"title":"Action Selection"}
		if (chance(EPSILON))
			a = randompick(As);
		else
			a = argmax(As);
		//:end edit
		return a

	}

	my.update = function(s, a, r, s_){
		var difference
		var new_Q
		//:edit {"title":"Update"}
		delta     = r + 0.9 * valmax(Q.get(s_)) - Q.get(s, a)
		new_value = Q.get(s, a) + ALPHA * delta
		Q.set(s, a, new_value)
		//:end
	}

	my.start_episode = function(){
		my.task.reset();
	}

	my.run_episode = function(){
		my.start_episode();
		while(!task.ended()){
			my.do_step();
		}
		my.start_episode()
	}

	my.do_step = function(){
		var s = task.getState();
		var a = my.action_select(s)
		var r = task.act(a);
		var s_ = task.getState();

		my.update(s, a, r, s_);

		if (task.ended()){
			console.log("Episode finished in " + my.task.step + " steps, return: " + my.task.R + ".");

			my.episode ++;
	        
	        my.R.push([my.episode, my.task.R]);
	        my.steps.push([my.episode, my.task.step])
	        $.plot(my.panels[0], [{data:my.R, label:'Return'},
	        					  {data:my.steps, label:'Timesteps'}]);

		}
	}
}



var run = function(my, run_num){

	var show_every = 10

	if(my.episode % show_every == 0 && my.episode > 0)
	{

		if (my.buttons['Next'].attr('data-justclicked') == 'true') {
			my.do_step();
			my.task.render(my.Q);
		}

		if (my.buttons['Play'].attr('data-justclicked') == 'true') {
			my.autoplay = true;			
		}

		if (my.autoplay && run_num % 2 == 0) {
			my.do_step();
			my.task.render(my.Q);
		}

	} else {
		my.autoplay = false;
		my.run_episode();
		my.task.render(my.Q);
	}

	if (my.episode > my.EPISODES){
		return true;
	}
}
</script>

<script>
    prj = new projector.Projector($('body'));
</script>

</body>
</html>