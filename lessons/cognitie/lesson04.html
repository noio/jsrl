<html>
    <head>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min.js"></script>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/flot/0.8/jquery.flot.min.js"></script>
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/flot/0.8/jquery.flot.errorbars.min.js"></script>
        <script type="text/javascript" src="../../js/lib/klass.min.js"></script>

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <script type="text/javascript" src="../../js/lib/codemirror.min.js"></script>
        <link type="text/css" href="../../js/lib/codemirror.css" media="all" rel="stylesheet">

        <script type="text/javascript" src="../../js/utils.js"></script>
        <script type="text/javascript" src="../../js/learning.js"></script>
        <script type="text/javascript" src="../../js/gridworld.js"></script>
        <link href="../../js/gridworld.css" media="all" rel="stylesheet" type="text/css">

        <script type="text/javascript" src="../../js/projector.js"></script>
        
        <link href="../../style.css" media="all" rel="stylesheet" type="text/css">

        <title>Cognitie Les 3</title>

    </head>
    <body>
        <h1>Practicum Cognitie</h1>
        <div id='projector-description'>
			<h2>Q learning en \(\gamma\)</h2>
			<p>In de vorige les hebben we gezien dat we in de Q-functie
				een parameter \(gamma\) hebben, die reward ``in de toekomst'' minder zwaar
				telt dan onmiddelijke reward. In dit practicum gaan we zien waarom deze parameter
				handig is en in welke situaties.</p>
				
			<p>Rechts zie je een aantal nieuwe functionaliteiten, ten eerste kun je nu
				zien hoe de kaart wordt opgebouwd in `World'. De <em>a</em>-tjes staan voor
				`moerassen': hier kan de agent doorheen rijden, maar hij heeft een kans van 1 op
				2 dat hij het niet overleeft en dat de episode ten einde komt en hij een negatieve
				reward van -15 krijgt.</p>
				<p>Verder heb je drie soorten reward-tegels:</p>
				<ul>
					<li><strong>x</strong>: een kleine reward van 10.
					<li><strong>y</strong>: een medium reward van 20.
					<li><strong>z</strong>: een grote reward van 30.
				</ul>
			<p>Ook zie je een nieuwe constante PROB_WORLD_END. Deze constante geeft de kans aan
				dat in een willekeurige stap de wereld vergaat. Als je je reward wilt maximaliseren
				moet je hier rekening mee houden: als je in 10 stappen een hele grote reward kan krijgen,
				maar de kans is groot dat voor die tijd de wereld vergaat, dan wil je misschien
				liever een reward pakken die kleiner is, maar dichterbij ligt,</p>
			<p class="question"><span>1a)</span> Stel dat er een reward van 100 op
				10 stappen ligt en een reward van 50 op 3 stappen. Nadat je een reward geclaimd
				hebt eindigt de episode. De kans dat de wereld ondertussen vergaat
				is elke stap 0.1. Hoe groot is de kans dat je 10 stappen kan zetten voordat de wereld vergaat?
				Als je verwachtte reward \(E[r]\) wilt maximaliseren, naar welke reward kun je dan het
				beste toelopen?</p>
			<p class="question"><span>1b)</span> De kans dat de wereld vergaat is nu 0.3.
				Draai de simulatie en kijk wat er gebeurt.
				Naar welk reward-eindpunt loopt de agent nu?</p>
			<p class="question"><span>2a)</span> Zet de kans dat de wereld vergaat nu op
				0.0. Waar gaat de agent nu naar toe?</p>
				<p class="question"><span>2b)</span> Is dit de optimale keuze?</p>				
				<p class="question"><span>2c)</span> Welke parameter kun je aanpassen 
					om de agent wel de optimale keuze te laten maken? Werkt dit?</p>				
		<p class="question"><span>3a)</span> Laat de parameters hetzeflde, maar zet nu STEP_REWARD op -10. 
			Wat betekent dit?</p>
			<p class="question"><span>3b)</span> Hoe verandert het gedrag van de agent nu?</p>
			<p class="question"><span>3c)</span> Wat is het fundamentele verschil tussen het aanpassen
				van de STEP_REWARD en de GAMMA-parameter?</p>
			

        </div>

<script id="projector-script">
/**
 * This will run immediately upon load.
 * Put any variables you want to save into 'my'.
 */

var setup = function(my){
	my.panels = projector.createPanels([1,2]);
	my.buttons = projector.createButtons(["Next", "Play"])
	
	var task_world
	var task_world = [
	"z _ _ _ _ _",
	"_ _ _ _ _ _",
	"_ _ _ _ _ x",
	"_ _ _ s _ _",	
	]
	my.task = new gridworld.GridWorld(task_world)
	my.task.setpanel(my.panels[1])
	my.task.STEP_REWARD = -1;
	my.autoplay = false
	my.task.render();
}

var first = function(my){
	var ALPHA
	var GAMMA
	var EPSILON
	var STEP_REWARD
	var PROB_WORLD_END
	var EPISODES
	var task_world
	
	//:edit {"title": "World"}
	task_world = [
	"z _ _ _ _ _",
	"_ _ _ _ _ _",
	"_ _ _ _ _ x",
	"_ _ _ s _ _",	
	]
	//:end edit
	my.task = new gridworld.GridWorld(task_world)
	my.task.setpanel(my.panels[1])
	my.task.STEP_REWARD = -1;
	my.autoplay = false
	my.task.render();

	//:edit {"title":"Variables"}
	ALPHA = 0.5
	GAMMA = 0.7
	EPSILON = 1.0
	PROB_WORLD_END = 0.3
	EPISODES = 1000
	STEP_REWARD = -1
	//:end edit
	my.task.STEP_REWARD = STEP_REWARD

	my.ALPHA = ALPHA
	my.GAMMA = GAMMA
	my.EPISODES = EPISODES
	
	my.task.END_PROBABILITY = PROB_WORLD_END;

	my.Q = new StateActionValueTable()
	var Q = my.Q
	var task = my.task

	//:edit {"title":"Initialization"}
	Q.fill(task.states(), task.actions(), 0.0)
	//:end edit

	// Other persistent variables
	my.steps = []
	my.rewards = []
	my.episode = 0
	my.step = 0;
	my.total_reward = 0;

	// Define functions

	my.action_select = function(s){
		var a
		var As = Q.get(s)

		//:edit {"title":"Action Selection"}
		if (chance(EPSILON))
			a = randompick(As);
		else
			a = argmax(As);
		//:end edit
		return a

	}

	my.update = function(s, a, r, s_){
		var difference
		var new_Q
		//:edit {"title":"Update"}
		difference = r + GAMMA * valmax(Q.get(s_))
		new_Q = (1- ALPHA) * Q.get(s, a) + ALPHA * difference
		Q.set(s, a, new_Q)
		EPSILON = 0.999 * EPSILON
		//:end
	}

	my.start_episode = function(){
		task.reset();
		my.step = 0;
		my.episode_reward = 0;
	}

	my.run_episode = function(){
		my.start_episode();
		while(!task.ended()){
			my.do_step();
		}
		my.start_episode()
	}

	my.do_step = function(){
		var s = task.getState();

		var a = my.action_select(s)

		var r = task.act(a);
		
		my.episode_reward += r
		
		var s_ = task.getState();
		
		
		my.update(s, a, r, s_);
		my.step ++;
		
		if(Math.random() < my.PROB_WORLD_END)
		{
			my.task.finished = true;
			console.log("APOCALYPSE");
		}


		if (task.ended()){
			console.log("Episode finished in " + my.step + " steps with " + my.episode_reward +" reward .");

			my.steps.push([my.episode, my.step]);
				
			my.total_reward += my.episode_reward;
			
			my.episode ++;
			my.rewards.push([my.episode, my.total_reward / my.episode])

			$.plot(my.panels[0], [{'data':my.rewards,'label':'mean reward', color:'red'}]);
		}
	}
}



var run = function(my, run_num){

	N = 100

	if(my.episode % N == 5 && false)
	{

		if (my.buttons['Next'].attr('data-justclicked') == 'true') {
			my.do_step();
			my.task.render(my.Q);
		}

		if (my.buttons['Play'].attr('data-justclicked') == 'true') {
			my.autoplay = true;			
		}

		if (my.autoplay && run_num % 5 == 0) {
			my.do_step();
			my.task.render(my.Q);
		}

	} else {
		my.autoplay = false;
		my.run_episode();
		my.task.render(my.Q);
	}

	if (my.episode > my.EPISODES){
		return true;
	}
}
</script>

<script>
    prj = new projector.Projector($('body'));
</script>

</body>
</html>